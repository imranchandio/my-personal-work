# p6_data_pipeline_details_config.yaml

# ======================================
# BASIC COMPARTMENT / ENV CONFIG
# ======================================
compartment_id: "ocid1.compartment.oc1..aaaaaaaagqvd5cybmsllqqkbligbbij6oueegrkrjyusjfikqnvdpovs0laq"

shared_dag_config:
  retries: 3
  retry_delay: 300          # seconds
  max_active_runs: 1
  catchup: false
  start_date: "2024-11-20T00:00:00"   # ISO 8601
  run_poll_interval: 60               # seconds between status checks

# ======================================
# SHARED DATA FLOW SETTINGS (NO POOL)
# ======================================
shared_dataflow_config:
  driver_shape: "VM.Standard.E4.Flex"
  executor_shape: "VM.Standard.E4.Flex"

  driver_shape_config:
    ocpus: 2
    memory_in_gbs: 16

  executor_shape_config:
    ocpus: 2
    memory_in_gbs: 16

  num_executors: 2

  logs_bucket_uri: "oci://bkt-neom-enowa-synergyze-dev-data-operations@axjj8sdvrg1w/dataflow-applications/logs/"

# ======================================
# DATA FLOW APPLICATION DEFINITION
# ======================================
applications:
  details:
    p6_simple_app:
      display_name: "df_app_p6_simple_dataflow"

      # >>> PUT YOUR REAL DATA FLOW APPLICATION OCID HERE <<<
      # example prefix: ocid1.dataflowapplication.oc1.me-jeddah-1.anvgkljrs...
      application_id: "ocid1.dataflowapplication.oc1.me-jeddah-1.YOUR_FULL_APP_OCID_HERE"

      # Optional: arguments passed to the Spark job
      arguments: []

dataflow_app:
  - application_key: "p6_simple_app"
    logs_bucket_uri: "oci://bkt-neom-enowa-synergyze-dev-data-operations@axjj8sdvrg1w/dataflow-applications/logs/"

# ======================================
# DAG CONFIGS (NO SPACES / SPECIAL CHARS)
# ======================================
dag_configs:
  aggregation:
    dag_id: "airflow_dag_p6_pds_to_oci_adw"
    description: "P6 PDS to OCI ADW DataFlow pipeline"
    schedule_interval: "0 2 * * *"
    retries: 3
    retry_delay: 300
    max_active_runs: 1
    catchup: false
    run_poll_interval: 60
    tasks:
      - "p6_simple_app"

  consumption:
    dag_id: "airflow_dag_p6_data_consumption"
    description: "P6 data consumption DataFlow pipeline"
    schedule_interval: "0 3 * * *"
    retries: 3
    retry_delay: 300
    max_active_runs: 1
    catchup: false
    run_poll_interval: 60
    tasks:
      - "p6_simple_app"
